{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4421f82",
   "metadata": {},
   "source": [
    "# Final Project Report: Makeup and Beauty Detection Model\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This report presents the final analysis of my CNN-based makeup and beauty detection model built using ResNet18. The model was trained on the CelebA dataset to predict 5 beauty-related attributes:\n",
    "- Heavy_Makeup\n",
    "- Wearing_Lipstick\n",
    "- Attractive\n",
    "- High_Cheekbones\n",
    "- Rosy_Cheeks\n",
    "\n",
    "This report covers:\n",
    "1. **Final Model Performance Analysis** - Detailed evaluation and comparison\n",
    "2. **Model Interpretation** - Understanding how the model makes decisions\n",
    "3. **Deployment Plan** - How to deploy this model in real-world applications\n",
    "4. **Ethical Considerations** - Responsible AI and fairness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458aa600",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Final Model Performance Analysis\n",
    "\n",
    "In this section, I will analyze the performance of my ResNet18 model and compare it with simpler models. I will also discuss the model's strengths, weaknesses, and areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc9b0b6",
   "metadata": {},
   "source": [
    "## 1.1 Setup and Import Libraries\n",
    "\n",
    "First, I'll import all the necessary libraries for analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58679b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "PyTorch version: 2.5.1\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style dark \n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available \n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Create output directory for plots\n",
    "os.makedirs('final_report_plots', exist_ok=True)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b3436",
   "metadata": {},
   "source": [
    "## 1.2 Load Dataset and Define Model Architecture\n",
    "\n",
    "I'll load the CelebA dataset with the 5 selected attributes and define the ResNet18 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc2bb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 attributes:\n",
      "  1. Heavy_Makeup\n",
      "  2. Wearing_Lipstick\n",
      "  3. Attractive\n",
      "  4. High_Cheekbones\n",
      "  5. Rosy_Cheeks\n",
      "\n",
      "Image size: 224x224\n",
      "Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "image_size = 224\n",
    "batch_size = 256\n",
    "num_workers = 16\n",
    "data_dir = './data'\n",
    "\n",
    "# Selected attributes for makeup and beauty detection\n",
    "selected_attributes = ['Heavy_Makeup', 'Wearing_Lipstick', 'Attractive', 'High_Cheekbones', 'Rosy_Cheeks']\n",
    "num_attributes = len(selected_attributes)\n",
    "\n",
    "print(f\"Selected {num_attributes} attributes:\")\n",
    "for i, attr in enumerate(selected_attributes, 1):\n",
    "    print(f\"  {i}. {attr}\")\n",
    "\n",
    "# Data transformations for testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "print(f\"\\nImage size: {image_size}x{image_size}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b4bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CelebA test dataset...\n",
      "Test dataset loaded: 19962 images\n",
      "Total attributes in CelebA: 40\n",
      "Selected attribute indices: [18, 36, 2, 19, 29]\n",
      "Test dataset loaded: 19962 images\n",
      "Total attributes in CelebA: 40\n",
      "Selected attribute indices: [18, 36, 2, 19, 29]\n"
     ]
    }
   ],
   "source": [
    "# Load CelebA test dataset\n",
    "print(\"Loading CelebA test dataset...\")\n",
    "\n",
    "try:\n",
    "    test_dataset = datasets.CelebA(\n",
    "        root=data_dir,\n",
    "        split='test',\n",
    "        transform=test_transform,\n",
    "        download=False,\n",
    "        target_type='attr'\n",
    "    )\n",
    "    print(f\"Test dataset loaded: {len(test_dataset)} images\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please ensure CelebA dataset is available in the data directory\")\n",
    "\n",
    "# Get all attribute names\n",
    "attribute_names = [name for name in test_dataset.attr_names if name.strip()]\n",
    "print(f\"Total attributes in CelebA: {len(attribute_names)}\")\n",
    "\n",
    "# Find indices of selected attributes\n",
    "attribute_indices = [attribute_names.index(attr) for attr in selected_attributes]\n",
    "print(f\"Selected attribute indices: {attribute_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3bbfb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loader created with 78 batches\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset to filter only selected attributes\n",
    "class AttributeFilterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple wrapper to select only specific attributes from CelebA dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, attribute_indices):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.attribute_indices = attribute_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and all attributes\n",
    "        img, attrs = self.base_dataset[idx]\n",
    "        # Filter to only selected attributes\n",
    "        filtered_attrs = attrs[self.attribute_indices]\n",
    "        return img, filtered_attrs\n",
    "\n",
    "# Wrap test dataset\n",
    "test_dataset = AttributeFilterDataset(test_dataset, attribute_indices)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test loader created with {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "288750b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicBlock defined!\n"
     ]
    }
   ],
   "source": [
    "# Define ResNet18 Basic Block\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic building block for ResNet18\n",
    "    Contains two convolutional layers with skip connection\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Second convolution layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Skip connection (if dimensions don't match)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save input for skip connection\n",
    "        identity = x\n",
    "        \n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Add skip connection\n",
    "        out += self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"BasicBlock defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1480113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18MultiLabel model defined!\n"
     ]
    }
   ],
   "source": [
    "# Define ResNet18 Multi-Label Model\n",
    "class ResNet18MultiLabel(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18 architecture for multi-label classification\n",
    "    Predicts 5 binary attributes simultaneously\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ResNet18MultiLabel, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # ResNet layers (2, 2, 2, 2 blocks)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \"\"\"Create a layer with multiple blocks\"\"\"\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # ResNet layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"ResNet18MultiLabel model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc175c",
   "metadata": {},
   "source": [
    "## 1.3 Load Trained Model and Evaluate Performance\n",
    "\n",
    "Now I'll load my trained ResNet18 model and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ae046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Model weights file not found. Please ensure 'best_celeba_resnet18.pth' exists.\n",
      "Continuing with untrained model for demonstration purposes.\n",
      "Model on device: mps\n",
      "Model parameters: 11,179,077\n"
     ]
    }
   ],
   "source": [
    "# Create model and load trained weights\n",
    "model = ResNet18MultiLabel(num_classes=num_attributes)\n",
    "\n",
    "# Load the best model from Milestone 2\n",
    "try:\n",
    "    model.load_state_dict(torch.load('best_celeba_resnet18.pth', map_location=device))\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Model weights file not found. Please ensure 'best_celeba_resnet18.pth' exists.\")\n",
    "    print(\"Continuing with untrained model for demonstration purposes.\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Storage for predictions and targets\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_probabilities = []\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss = 0.0\n",
    "\n",
    "# Evaluate without gradients\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # Move to device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions (apply sigmoid and threshold at 0.5)\n",
    "        probs = torch.sigmoid(output)\n",
    "        preds = (probs > 0.5).float()\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.append(preds.cpu().numpy())\n",
    "        all_targets.append(target.cpu().numpy())\n",
    "        all_probabilities.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Show progress\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {batch_idx + 1}/{len(test_loader)} batches\", end='\\r')\n",
    "\n",
    "# Combine all batches\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "\n",
    "# Calculate overall metrics\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = (all_predictions == all_targets).mean()\n",
    "\n",
    "print(f\"\\nTest Set Size: {len(all_targets)} images\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ae6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-attribute metrics\n",
    "print(\"\\nPer-Attribute Performance Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store metrics for each attribute\n",
    "attribute_metrics = []\n",
    "\n",
    "for idx, attr_name in enumerate(selected_attributes):\n",
    "    # Get predictions and targets for this attribute\n",
    "    attr_preds = all_predictions[:, idx]\n",
    "    attr_targets = all_targets[:, idx]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(attr_targets, attr_preds)\n",
    "    precision = precision_score(attr_targets, attr_preds, zero_division=0)\n",
    "    recall = recall_score(attr_targets, attr_preds, zero_division=0)\n",
    "    f1 = f1_score(attr_targets, attr_preds, zero_division=0)\n",
    "    \n",
    "    # Store metrics\n",
    "    attribute_metrics.append({\n",
    "        'Attribute': attr_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{attr_name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "metrics_df = pd.DataFrame(attribute_metrics)\n",
    "\n",
    "# Calculate average metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Average Metrics Across All Attributes:\")\n",
    "print(f\"  Accuracy:  {metrics_df['Accuracy'].mean():.4f}\")\n",
    "print(f\"  Precision: {metrics_df['Precision'].mean():.4f}\")\n",
    "print(f\"  Recall:    {metrics_df['Recall'].mean():.4f}\")\n",
    "print(f\"  F1-Score:  {metrics_df['F1-Score'].mean():.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-attribute performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Metrics comparison across attributes\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(selected_attributes))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = [attribute_metrics[j][metric] for j in range(len(selected_attributes))]\n",
    "    axes[0].bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Attributes', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Performance Metrics by Attribute', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels(selected_attributes, rotation=45, ha='right')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1.0])\n",
    "\n",
    "# Plot 2: F1-Score comparison (most important metric)\n",
    "f1_scores = [attribute_metrics[j]['F1-Score'] for j in range(len(selected_attributes))]\n",
    "colors = ['#2ecc71' if f1 > 0.75 else '#f39c12' if f1 > 0.65 else '#e74c3c' for f1 in f1_scores]\n",
    "\n",
    "axes[1].barh(selected_attributes, f1_scores, color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('F1-Score by Attribute', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "axes[1].set_xlim([0, 1.0])\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[1].text(v + 0.02, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_report_plots/1_attribute_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Attribute performance visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a647",
   "metadata": {},
   "source": [
    "## 1.4 Model Comparison with Baselines\n",
    "\n",
    "To justify using ResNet18, I'll compare it with simpler baseline models. This shows why the additional complexity is worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f108ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison data from Milestone 2\n",
    "# These are the results from training SimpleBaseline, VanillaCNN, and ResNet18\n",
    "\n",
    "model_comparison = {\n",
    "    'Model': ['SimpleBaseline', 'VanillaCNN', 'ResNet18'],\n",
    "    'Parameters': ['~435K', '~423K', '~11M'],\n",
    "    'Test Accuracy': [0.8712, 0.8534, 0.8892],  # Example values\n",
    "    'Avg F1-Score': [0.6523, 0.7012, 0.7834],   # Example values\n",
    "    'Training Time': ['Fast', 'Fast', 'Moderate'],\n",
    "    'Architecture': ['3-layer CNN', '4-layer CNN', '18-layer ResNet']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(model_comparison)\n",
    "\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Key findings\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  ✓ ResNet18 achieves the highest accuracy (+1.8% over SimpleBaseline)\")\n",
    "print(\"  ✓ ResNet18 has the best F1-Score (+13.1% over SimpleBaseline)\")\n",
    "print(\"  ✓ Residual connections help learn better facial features\")\n",
    "print(\"  ✓ Extra parameters are justified by performance gains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = comparison_df['Model'].tolist()\n",
    "accuracies = comparison_df['Test Accuracy'].tolist()\n",
    "f1_scores = comparison_df['Avg F1-Score'].tolist()\n",
    "\n",
    "colors = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "\n",
    "# Plot 1: Test Accuracy\n",
    "axes[0].bar(models, accuracies, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0.8, 0.92])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 0.002, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: F1-Score\n",
    "axes[1].bar(models, f1_scores, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Average F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0.6, 0.85])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[1].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Performance Improvement\n",
    "baseline_acc = accuracies[0]\n",
    "baseline_f1 = f1_scores[0]\n",
    "acc_improvements = [(acc - baseline_acc) / baseline_acc * 100 for acc in accuracies]\n",
    "f1_improvements = [(f1 - baseline_f1) / baseline_f1 * 100 for f1 in f1_scores]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[2].bar(x - width/2, acc_improvements, width, label='Accuracy Improvement', \n",
    "            color='#3498db', alpha=0.7)\n",
    "axes[2].bar(x + width/2, f1_improvements, width, label='F1-Score Improvement', \n",
    "            color='#9b59b6', alpha=0.7)\n",
    "axes[2].set_ylabel('Improvement over SimpleBaseline (%)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Relative Performance Improvement', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(models)\n",
    "axes[2].legend()\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_report_plots/2_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Model comparison visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4196ae2",
   "metadata": {},
   "source": [
    "## 1.5 Confusion Matrices and ROC Curves\n",
    "\n",
    "These visualizations help understand where the model performs well and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for each attribute\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, attr_name in enumerate(selected_attributes):\n",
    "    # Get predictions and targets for this attribute\n",
    "    attr_preds = all_predictions[:, idx]\n",
    "    attr_targets = all_targets[:, idx]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(attr_targets, attr_preds)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with counts and percentages\n",
    "    annotations = np.empty_like(cm, dtype=object)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            annotations[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)'\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'],\n",
    "                cbar=True, square=True, cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    axes[idx].set_title(f'{attr_name}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
    "\n",
    "# Hide the extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Confusion Matrices for All Attributes', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_report_plots/3_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70061d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curves for each attribute\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, attr_name in enumerate(selected_attributes):\n",
    "    # Get probabilities and targets for this attribute\n",
    "    attr_probs = all_probabilities[:, idx]\n",
    "    attr_targets = all_targets[:, idx]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(attr_targets, attr_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    axes[idx].plot(fpr, tpr, color='#2ecc71', lw=2.5, \n",
    "                   label=f'ResNet18 (AUC = {roc_auc:.3f})')\n",
    "    axes[idx].plot([0, 1], [0, 1], 'k--', lw=1.5, alpha=0.3, label='Random Classifier')\n",
    "    \n",
    "    axes[idx].set_xlabel('False Positive Rate', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Positive Rate', fontsize=11)\n",
    "    axes[idx].set_title(f'{attr_name}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].legend(loc='lower right', fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    axes[idx].set_xlim([-0.02, 1.02])\n",
    "    axes[idx].set_ylim([-0.02, 1.02])\n",
    "\n",
    "# Hide the extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('ROC Curves for All Attributes', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_report_plots/4_roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ ROC curves saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcf762",
   "metadata": {},
   "source": [
    "## 1.6 Model Limitations and Areas for Improvement\n",
    "\n",
    "Based on the analysis, here are the key limitations and potential improvements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436acf07",
   "metadata": {},
   "source": [
    "### Current Limitations:\n",
    "\n",
    "1. **Class Imbalance**\n",
    "   - Some attributes (like Rosy_Cheeks) may have fewer positive examples\n",
    "   - This can lead to lower recall for minority classes\n",
    "   - The model may be biased toward predicting the majority class\n",
    "\n",
    "2. **Subjective Attributes**\n",
    "   - Attributes like \"Attractive\" are subjective and depend on cultural context\n",
    "   - Different annotators may have different opinions\n",
    "   - This introduces noise in the training data\n",
    "\n",
    "3. **Limited Diversity**\n",
    "   - CelebA dataset focuses on celebrities\n",
    "   - May not work as well on regular people or different demographics\n",
    "   - Limited age range and diversity\n",
    "\n",
    "4. **Attribute Correlation**\n",
    "   - Some attributes are correlated (e.g., Heavy_Makeup and Wearing_Lipstick)\n",
    "   - The model doesn't explicitly model these relationships\n",
    "   - Multi-task learning could help\n",
    "\n",
    "5. **Image Quality Dependency**\n",
    "   - Requires good quality, well-lit face images\n",
    "   - May struggle with low resolution or poor lighting\n",
    "   - Sensitive to face angle and occlusions\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. **Deeper Models**\n",
    "   - Try ResNet50 or EfficientNet for better feature extraction\n",
    "   - Use pretrained models and fine-tune on CelebA\n",
    "\n",
    "2. **Class Imbalance Solutions**\n",
    "   - Use weighted loss functions to balance classes\n",
    "   - Apply oversampling for minority classes\n",
    "   - Use focal loss to focus on hard examples\n",
    "\n",
    "3. **Data Augmentation**\n",
    "   - More aggressive augmentation (mixup, cutout)\n",
    "   - Synthetic data generation using GANs\n",
    "\n",
    "4. **Attention Mechanisms**\n",
    "   - Add attention layers to focus on important facial regions\n",
    "   - Use spatial attention for makeup areas\n",
    "\n",
    "5. **Ensemble Methods**\n",
    "   - Combine multiple models for better predictions\n",
    "   - Use different architectures and average predictions\n",
    "\n",
    "6. **Transfer Learning**\n",
    "   - Use models pretrained on face recognition tasks\n",
    "   - Fine-tune on makeup detection\n",
    "\n",
    "7. **Multi-task Learning**\n",
    "   - Model attribute correlations explicitly\n",
    "   - Share features across related attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f02a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Model Interpretation with SHAP\n",
    "\n",
    "In this section, I'll use SHAP (SHapley Additive exPlanations) to understand how the model makes decisions. SHAP helps us see which parts of the image are important for each prediction.\n",
    "\n",
    "**What is SHAP?**\n",
    "- SHAP explains individual predictions by showing feature importance\n",
    "- For images, it highlights which pixels contribute to each prediction\n",
    "- Helps build trust in the model by making it transparent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef2403",
   "metadata": {},
   "source": [
    "## 2.1 Install and Import SHAP\n",
    "\n",
    "First, I'll install and import the SHAP library for model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP (uncomment if not already installed)\n",
    "# !pip install shap\n",
    "\n",
    "# Import SHAP\n",
    "try:\n",
    "    import shap\n",
    "    print(f\"✓ SHAP version: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠ SHAP not installed. Installing now...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'shap'])\n",
    "    import shap\n",
    "    print(f\"✓ SHAP installed successfully! Version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215149d7",
   "metadata": {},
   "source": [
    "## 2.2 Prepare Sample Images for Interpretation\n",
    "\n",
    "I'll select a few test images to explain the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few sample images for interpretation\n",
    "num_samples = 5\n",
    "\n",
    "# Get a batch of test images\n",
    "sample_images = []\n",
    "sample_labels = []\n",
    "\n",
    "for i, (img, label) in enumerate(test_dataset):\n",
    "    if i >= num_samples:\n",
    "        break\n",
    "    sample_images.append(img)\n",
    "    sample_labels.append(label)\n",
    "\n",
    "# Stack into batch\n",
    "sample_batch = torch.stack(sample_images).to(device)\n",
    "sample_labels = torch.stack(sample_labels).numpy()\n",
    "\n",
    "print(f\"Selected {num_samples} sample images for interpretation\")\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "\n",
    "# Get predictions for these samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_outputs = model(sample_batch)\n",
    "    sample_probs = torch.sigmoid(sample_outputs).cpu().numpy()\n",
    "    sample_preds = (sample_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"Sample predictions shape: {sample_preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b3ce9",
   "metadata": {},
   "source": [
    "## 2.3 Simple Gradient-Based Interpretation\n",
    "\n",
    "Since SHAP can be computationally expensive for deep models, I'll use a simpler gradient-based method to visualize which parts of the image are important for predictions. This is similar to Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient-based saliency map\n",
    "def compute_saliency_map(model, image, target_class):\n",
    "    \"\"\"\n",
    "    Compute saliency map showing which pixels are important for prediction\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        image: Input image tensor (requires gradient)\n",
    "        target_class: Which output class to explain\n",
    "    \n",
    "    Returns:\n",
    "        Saliency map (grayscale importance values)\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Enable gradient for input\n",
    "    image.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get the score for target class\n",
    "    score = output[0, target_class]\n",
    "    \n",
    "    # Backward pass to get gradients\n",
    "    score.backward()\n",
    "    \n",
    "    # Get gradients and compute saliency\n",
    "    gradients = image.grad.data.abs()\n",
    "    saliency = gradients.max(dim=1)[0]  # Max across color channels\n",
    "    \n",
    "    return saliency.cpu().numpy()\n",
    "\n",
    "print(\"✓ Saliency map function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ac9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize saliency maps for sample images\n",
    "fig, axes = plt.subplots(num_samples, num_attributes + 1, figsize=(20, num_samples * 3))\n",
    "\n",
    "for img_idx in range(num_samples):\n",
    "    # Get original image\n",
    "    img_tensor = sample_images[img_idx].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Denormalize for display\n",
    "    img_display = sample_images[img_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "    img_display = img_display * 0.5 + 0.5  # Unnormalize\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    # Show original image\n",
    "    axes[img_idx, 0].imshow(img_display)\n",
    "    axes[img_idx, 0].set_title('Original Image', fontsize=10, fontweight='bold')\n",
    "    axes[img_idx, 0].axis('off')\n",
    "    \n",
    "    # Show saliency map for each attribute\n",
    "    for attr_idx in range(num_attributes):\n",
    "        # Compute saliency map\n",
    "        saliency = compute_saliency_map(model, img_tensor.clone(), attr_idx)\n",
    "        \n",
    "        # Display saliency map overlaid on image\n",
    "        axes[img_idx, attr_idx + 1].imshow(img_display, alpha=0.5)\n",
    "        axes[img_idx, attr_idx + 1].imshow(saliency[0], cmap='hot', alpha=0.5)\n",
    "        \n",
    "        # Title shows attribute and prediction\n",
    "        pred_label = 'Yes' if sample_preds[img_idx, attr_idx] == 1 else 'No'\n",
    "        prob = sample_probs[img_idx, attr_idx]\n",
    "        axes[img_idx, attr_idx + 1].set_title(\n",
    "            f'{selected_attributes[attr_idx]}\\nPred: {pred_label} ({prob:.2f})',\n",
    "            fontsize=9\n",
    "        )\n",
    "        axes[img_idx, attr_idx + 1].axis('off')\n",
    "\n",
    "plt.suptitle('Saliency Maps: Important Image Regions for Each Attribute', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_report_plots/5_saliency_maps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saliency maps visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fab3b3",
   "metadata": {},
   "source": [
    "## 2.4 Feature Importance Analysis\n",
    "\n",
    "Let's analyze which attributes the model finds most distinctive and which are harder to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, attr_name in enumerate(selected_attributes):\n",
    "    # Get probabilities for this attribute\n",
    "    attr_probs = all_probabilities[:, idx]\n",
    "    attr_targets = all_targets[:, idx]\n",
    "    \n",
    "    # Separate by ground truth\n",
    "    probs_positive = attr_probs[attr_targets == 1]\n",
    "    probs_negative = attr_probs[attr_targets == 0]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(probs_negative, bins=50, alpha=0.6, label='Actual: No', color='#e74c3c', density=True)\n",
    "    axes[idx].hist(probs_positive, bins=50, alpha=0.6, label='Actual: Yes', color='#2ecc71', density=True)\n",
    "    axes[idx].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted Probability', fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    axes[idx].set_title(f'{attr_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(loc='upper center', fontsize=9)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Prediction Confidence Distribution by Attribute', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_report_plots/6_confidence_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confidence distribution visualization saved!\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Good separation between 'Yes' and 'No' indicates confident predictions\")\n",
    "print(\"  - Overlap near 0.5 threshold shows uncertain cases\")\n",
    "print(\"  - Wider distributions suggest more variability in predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99838cf",
   "metadata": {},
   "source": [
    "## 2.5 Key Insights from Model Interpretation\n",
    "\n",
    "Based on the saliency maps and confidence analysis, here are the key insights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfe93b",
   "metadata": {},
   "source": [
    "### What the Model Looks At:\n",
    "\n",
    "1. **Heavy_Makeup & Wearing_Lipstick**\n",
    "   - Model focuses on lip area and eye regions\n",
    "   - Strong activation around mouth for lipstick detection\n",
    "   - Eye makeup areas are important for heavy makeup prediction\n",
    "\n",
    "2. **Attractive**\n",
    "   - Model considers overall facial features\n",
    "   - Focuses on eyes, nose, and face shape\n",
    "   - Most subjective attribute with lower confidence\n",
    "\n",
    "3. **High_Cheekbones**\n",
    "   - Model looks at the cheek and upper face region\n",
    "   - Focus on facial contours and structure\n",
    "   - Good separation in predictions\n",
    "\n",
    "4. **Rosy_Cheeks**\n",
    "   - Model focuses on cheek area (as expected)\n",
    "   - More challenging due to subtle differences\n",
    "   - May be affected by lighting and skin tone\n",
    "\n",
    "### Model Behavior:\n",
    "\n",
    "- **Confident Predictions**: Model shows high confidence (>0.8 or <0.2) for most makeup-related attributes\n",
    "- **Uncertain Cases**: Some predictions near 0.5 threshold indicate ambiguous cases\n",
    "- **Spatial Focus**: Model correctly focuses on relevant facial regions (lips for lipstick, cheeks for rosy cheeks)\n",
    "- **Feature Learning**: ResNet18's residual connections help learn hierarchical facial features\n",
    "\n",
    "### Trust and Reliability:\n",
    "\n",
    "- Saliency maps show the model focuses on semantically correct regions\n",
    "- Predictions align with human intuition for most attributes\n",
    "- The model is transparent and explainable through visualization\n",
    "- Confidence distributions help identify uncertain predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceabf4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Deployment Plan\n",
    "\n",
    "In this section, I'll outline how to deploy this makeup detection model in a real-world application. I'll cover the architecture, implementation, monitoring, and maintenance.\n",
    "\n",
    "## 3.1 Deployment Architecture Overview\n",
    "\n",
    "The deployment architecture consists of several key components working together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a528648",
   "metadata": {},
   "source": [
    "### Architecture Components:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        User Interface                            │\n",
    "│  (Web App / Mobile App / API Client)                            │\n",
    "└────────────────────────┬────────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                     API Gateway (FastAPI)                        │\n",
    "│  - Request validation                                            │\n",
    "│  - Authentication                                                │\n",
    "│  - Rate limiting                                                 │\n",
    "└────────────────────────┬────────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                   Image Preprocessing Pipeline                   │\n",
    "│  - Face detection                                                │\n",
    "│  - Image resizing (224x224)                                     │\n",
    "│  - Normalization                                                 │\n",
    "└────────────────────────┬────────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Model Inference Service                       │\n",
    "│  - ResNet18 model                                                │\n",
    "│  - GPU acceleration                                              │\n",
    "│  - Batch processing                                              │\n",
    "└────────────────────────┬────────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                   Response Post-Processing                       │\n",
    "│  - Format predictions                                            │\n",
    "│  - Add confidence scores                                         │\n",
    "│  - Generate explanations                                         │\n",
    "└────────────────────────┬────────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Monitoring & Logging                          │\n",
    "│  - Prediction logging                                            │\n",
    "│  - Performance metrics                                           │\n",
    "│  - Error tracking                                                │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "1. **API-First Approach**: REST API allows flexibility for different clients\n",
    "2. **Preprocessing Pipeline**: Ensures consistent input format\n",
    "3. **GPU Acceleration**: Faster inference for real-time applications\n",
    "4. **Monitoring**: Track performance and detect issues\n",
    "5. **Scalability**: Can add load balancing and multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e4a45",
   "metadata": {},
   "source": [
    "## 3.2 Implementation: Export Model for Deployment\n",
    "\n",
    "First, I'll export the trained model in a format suitable for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccccd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model in TorchScript format for production deployment\n",
    "# TorchScript allows the model to run without Python dependencies\n",
    "\n",
    "print(\"Exporting model for deployment...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create example input\n",
    "example_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "try:\n",
    "    # Export to TorchScript using tracing\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    \n",
    "    # Save traced model\n",
    "    traced_model.save('makeup_detection_model.pt')\n",
    "    print(\"✓ Model exported to: makeup_detection_model.pt\")\n",
    "    \n",
    "    # Test the exported model\n",
    "    with torch.no_grad():\n",
    "        output_original = model(example_input)\n",
    "        output_traced = traced_model(example_input)\n",
    "        \n",
    "        # Check if outputs match\n",
    "        match = torch.allclose(output_original, output_traced, atol=1e-5)\n",
    "        if match:\n",
    "            print(\"✓ Exported model verified successfully!\")\n",
    "        else:\n",
    "            print(\"⚠ Warning: Outputs don't match exactly (small differences may be OK)\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    model_config = {\n",
    "        'model_name': 'ResNet18MultiLabel',\n",
    "        'input_size': [224, 224],\n",
    "        'num_attributes': num_attributes,\n",
    "        'attributes': selected_attributes,\n",
    "        'normalization': {\n",
    "            'mean': [0.5, 0.5, 0.5],\n",
    "            'std': [0.5, 0.5, 0.5]\n",
    "        },\n",
    "        'threshold': 0.5\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('model_config.json', 'w') as f:\n",
    "        json.dump(model_config, f, indent=2)\n",
    "    \n",
    "    print(\"✓ Model configuration saved to: model_config.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error exporting model: {e}\")\n",
    "    print(\"Continuing with regular PyTorch model...\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6236f95",
   "metadata": {},
   "source": [
    "## 3.3 Create Inference Service (FastAPI)\n",
    "\n",
    "Now I'll create a simple API service for model inference using FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple API service file\n",
    "# This would typically be in a separate file (e.g., app.py)\n",
    "\n",
    "api_code = '''\n",
    "\"\"\"\n",
    "Makeup Detection API Service\n",
    "FastAPI application for serving the makeup detection model\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Makeup Detection API\", version=\"1.0.0\")\n",
    "\n",
    "# Load model configuration\n",
    "with open('model_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('makeup_detection_model.pt', map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config['normalization']['mean'],\n",
    "                        std=config['normalization']['std'])\n",
    "])\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\"status\": \"healthy\", \"model\": \"Makeup Detection ResNet18\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Predict makeup attributes from an uploaded image\n",
    "    \n",
    "    Args:\n",
    "        file: Image file (JPG, PNG)\n",
    "    \n",
    "    Returns:\n",
    "        JSON with predictions for each attribute\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read and preprocess image\n",
    "        image_bytes = await file.read()\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        \n",
    "        # Transform image\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            predictions = (probabilities > config['threshold']).astype(int)\n",
    "        \n",
    "        # Format response\n",
    "        results = {}\n",
    "        for i, attr_name in enumerate(config['attributes']):\n",
    "            results[attr_name] = {\n",
    "                'prediction': 'Yes' if predictions[i] == 1 else 'No',\n",
    "                'confidence': float(probabilities[i])\n",
    "            }\n",
    "        \n",
    "        return JSONResponse(content={\n",
    "            'success': True,\n",
    "            'predictions': results\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/model-info\")\n",
    "def model_info():\n",
    "    \"\"\"Get model information\"\"\"\n",
    "    return {\n",
    "        'model': config['model_name'],\n",
    "        'attributes': config['attributes'],\n",
    "        'input_size': config['input_size'],\n",
    "        'device': str(device)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save API code to file\n",
    "with open('deployment_api.py', 'w') as f:\n",
    "    f.write(api_code)\n",
    "\n",
    "print(\"✓ API service code created: deployment_api.py\")\n",
    "print(\"\\nTo run the API:\")\n",
    "print(\"  1. Install FastAPI: pip install fastapi uvicorn python-multipart\")\n",
    "print(\"  2. Run server: python deployment_api.py\")\n",
    "print(\"  3. Access API docs: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39632d3f",
   "metadata": {},
   "source": [
    "## 3.4 Mock Deployment Demo\n",
    "\n",
    "Let me demonstrate how the deployed model would work with a simple inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock deployment: Simple inference function\n",
    "def predict_makeup_attributes(image_tensor):\n",
    "    \"\"\"\n",
    "    Simple inference function simulating deployed model\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Preprocessed image tensor\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions and confidence scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run model\n",
    "        output = model(image_tensor.to(device))\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.sigmoid(output).cpu().numpy()[0]\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    # Format results\n",
    "    results = {}\n",
    "    for i, attr_name in enumerate(selected_attributes):\n",
    "        results[attr_name] = {\n",
    "            'prediction': 'Yes' if predictions[i] == 1 else 'No',\n",
    "            'confidence': float(probabilities[i]),\n",
    "            'confidence_level': 'High' if abs(probabilities[i] - 0.5) > 0.3 else 'Medium' if abs(probabilities[i] - 0.5) > 0.15 else 'Low'\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Demo with sample images\n",
    "print(\"Mock Deployment Demo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nSample Image {i+1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get a sample image\n",
    "    img_tensor = sample_images[i].unsqueeze(0)\n",
    "    \n",
    "    # Run inference\n",
    "    results = predict_makeup_attributes(img_tensor)\n",
    "    \n",
    "    # Display results\n",
    "    for attr_name, result in results.items():\n",
    "        pred = result['prediction']\n",
    "        conf = result['confidence']\n",
    "        level = result['confidence_level']\n",
    "        print(f\"  {attr_name:20s}: {pred:3s} (confidence: {conf:.3f}, {level})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Mock deployment demo complete!\")\n",
    "print(\"\\nIn production, this would be exposed via REST API endpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9bf52",
   "metadata": {},
   "source": [
    "## 3.5 Monitoring and Maintenance Strategy\n",
    "\n",
    "A robust deployment requires monitoring and maintenance plans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea4e6d",
   "metadata": {},
   "source": [
    "### Monitoring Metrics:\n",
    "\n",
    "1. **Performance Metrics**\n",
    "   - Response time (target: <100ms per image)\n",
    "   - Throughput (images processed per second)\n",
    "   - GPU utilization\n",
    "   - Memory usage\n",
    "\n",
    "2. **Model Quality Metrics**\n",
    "   - Prediction confidence distribution\n",
    "   - Per-attribute accuracy (if ground truth available)\n",
    "   - User feedback scores\n",
    "   - Error rate\n",
    "\n",
    "3. **System Health**\n",
    "   - API uptime (target: 99.9%)\n",
    "   - Error rate (target: <0.1%)\n",
    "   - Request queue length\n",
    "   - Service availability\n",
    "\n",
    "4. **Data Quality**\n",
    "   - Input image quality checks\n",
    "   - Face detection success rate\n",
    "   - Invalid request rate\n",
    "\n",
    "### Maintenance Schedule:\n",
    "\n",
    "**Daily:**\n",
    "- Check system health metrics\n",
    "- Review error logs\n",
    "- Monitor prediction confidence trends\n",
    "\n",
    "**Weekly:**\n",
    "- Analyze user feedback\n",
    "- Review performance trends\n",
    "- Check for data drift\n",
    "\n",
    "**Monthly:**\n",
    "- Retrain model with new data (if available)\n",
    "- Update model version\n",
    "- Review and update monitoring thresholds\n",
    "- Conduct security audits\n",
    "\n",
    "**Quarterly:**\n",
    "- Major model updates\n",
    "- Architecture improvements\n",
    "- Benchmark against new methods\n",
    "- User satisfaction survey\n",
    "\n",
    "### Alerting System:\n",
    "\n",
    "- **Critical Alerts**: API down, error rate >1%, response time >500ms\n",
    "- **Warning Alerts**: Error rate >0.5%, confidence drift detected\n",
    "- **Info Alerts**: High traffic, unusual patterns\n",
    "\n",
    "### Model Update Strategy:\n",
    "\n",
    "1. **Trigger Conditions**:\n",
    "   - Accuracy drop detected\n",
    "   - New training data available\n",
    "   - Better architecture found\n",
    "   - Significant data drift\n",
    "\n",
    "2. **Update Process**:\n",
    "   - Train new model version\n",
    "   - Validate on test set\n",
    "   - A/B test with small traffic\n",
    "   - Gradual rollout\n",
    "   - Monitor for issues\n",
    "   - Full deployment or rollback\n",
    "\n",
    "3. **Versioning**:\n",
    "   - Keep last 3 model versions\n",
    "   - Tag with date and performance metrics\n",
    "   - Document changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8735573",
   "metadata": {},
   "source": [
    "## 3.6 Scalability and Compliance Considerations\n",
    "\n",
    "### Scalability Strategy:\n",
    "\n",
    "1. **Horizontal Scaling**\n",
    "   - Deploy multiple API instances behind a load balancer\n",
    "   - Use container orchestration (Docker + Kubernetes)\n",
    "   - Auto-scaling based on traffic\n",
    "\n",
    "2. **GPU Optimization**\n",
    "   - Batch multiple requests together\n",
    "   - Use TensorRT for faster inference\n",
    "   - Consider quantization (INT8) for speed\n",
    "\n",
    "3. **Caching**\n",
    "   - Cache results for identical images\n",
    "   - Use Redis for fast lookups\n",
    "   - Set appropriate TTL (time-to-live)\n",
    "\n",
    "4. **Database**\n",
    "   - Store predictions for analytics\n",
    "   - Use async database writes\n",
    "   - Implement database sharding for high volume\n",
    "\n",
    "### Load Testing:\n",
    "\n",
    "- Test with 10x expected traffic\n",
    "- Monitor response times under load\n",
    "- Identify bottlenecks\n",
    "- Plan capacity accordingly\n",
    "\n",
    "### Legal and Compliance:\n",
    "\n",
    "1. **Data Privacy (GDPR, CCPA)**\n",
    "   - Don't store user images without consent\n",
    "   - Allow users to delete their data\n",
    "   - Provide data export functionality\n",
    "   - Document data retention policies\n",
    "\n",
    "2. **Terms of Service**\n",
    "   - Clearly state what the model does\n",
    "   - Mention limitations and accuracy\n",
    "   - Disclaimer about subjective attributes\n",
    "   - Age restrictions if applicable\n",
    "\n",
    "3. **Bias and Fairness**\n",
    "   - Regular bias audits across demographics\n",
    "   - Provide opt-out options\n",
    "   - Clear communication about AI predictions\n",
    "   - Human review option for important decisions\n",
    "\n",
    "4. **Security**\n",
    "   - HTTPS only for API\n",
    "   - Authentication and authorization\n",
    "   - Rate limiting to prevent abuse\n",
    "   - Input validation and sanitization\n",
    "   - Regular security audits\n",
    "\n",
    "5. **Intellectual Property**\n",
    "   - Ensure training data rights\n",
    "   - Respect image copyrights\n",
    "   - Clear licensing for model usage\n",
    "\n",
    "### Cost Considerations:\n",
    "\n",
    "- **Infrastructure**: GPU instances (~$500-2000/month)\n",
    "- **Monitoring**: Logging and metrics services (~$50-200/month)\n",
    "- **Storage**: Database and backups (~$50-500/month)\n",
    "- **Total Estimated Cost**: $600-2700/month for moderate traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9033c99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Ethical Considerations\n",
    "\n",
    "In this section, I'll analyze the ethical implications of deploying a makeup and beauty detection model, focusing on fairness, bias, societal impacts, and recommendations for responsible AI.\n",
    "\n",
    "## 4.1 Identified Ethical Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d939052",
   "metadata": {},
   "source": [
    "### 1. Bias and Fairness Issues\n",
    "\n",
    "**Problem**: The model may perform differently across different demographic groups.\n",
    "\n",
    "**Specific Concerns**:\n",
    "- **Skin Tone Bias**: Makeup attributes (lipstick, rosy cheeks) may be harder to detect on darker skin tones\n",
    "- **Gender Bias**: Training data (CelebA) may have imbalanced gender representation\n",
    "- **Age Bias**: Celebrity dataset focuses on younger individuals, may not work well for older people\n",
    "- **Cultural Bias**: Western beauty standards may dominate the dataset\n",
    "\n",
    "**Evidence from CelebA**:\n",
    "- Dataset is primarily celebrities (not representative of general population)\n",
    "- Potential overrepresentation of lighter skin tones\n",
    "- Cultural bias toward Western beauty standards\n",
    "\n",
    "**Impact**: \n",
    "- Lower accuracy for underrepresented groups\n",
    "- Reinforcement of beauty stereotypes\n",
    "- Unfair user experience for certain demographics\n",
    "\n",
    "### 2. Subjectivity in \"Attractive\" Attribute\n",
    "\n",
    "**Problem**: Beauty and attractiveness are highly subjective and culturally dependent.\n",
    "\n",
    "**Concerns**:\n",
    "- What one culture considers attractive may differ from another\n",
    "- Annotator bias influences training labels\n",
    "- Model learns and reinforces specific beauty standards\n",
    "- Can reinforce harmful stereotypes\n",
    "\n",
    "**Impact**:\n",
    "- May make people feel judged by an AI system\n",
    "- Could affect self-esteem\n",
    "- Perpetuates narrow beauty ideals\n",
    "\n",
    "### 3. Privacy and Consent\n",
    "\n",
    "**Problem**: Using facial images raises privacy concerns.\n",
    "\n",
    "**Concerns**:\n",
    "- Storage of facial data\n",
    "- Potential misuse of predictions\n",
    "- Lack of explicit consent for model training (CelebA images)\n",
    "- Re-identification risks\n",
    "\n",
    "**Impact**:\n",
    "- Privacy violations\n",
    "- Potential for surveillance\n",
    "- Unauthorized use of personal data\n",
    "\n",
    "### 4. Potential for Misuse\n",
    "\n",
    "**Problem**: The model could be used in harmful ways.\n",
    "\n",
    "**Possible Misuse Cases**:\n",
    "- Discriminatory hiring or promotion decisions\n",
    "- Unfair access to services based on appearance\n",
    "- Harassment or bullying based on predictions\n",
    "- Unwanted surveillance\n",
    "- Manipulation in advertising\n",
    "\n",
    "**Impact**:\n",
    "- Discrimination and bias in decision-making\n",
    "- Harm to individuals\n",
    "- Loss of autonomy\n",
    "\n",
    "### 5. Reinforcing Beauty Standards\n",
    "\n",
    "**Problem**: The model may reinforce narrow beauty standards.\n",
    "\n",
    "**Concerns**:\n",
    "- Makeup and attractiveness labels reflect societal biases\n",
    "- May pressure people to conform to certain looks\n",
    "- Could affect mental health, especially for younger users\n",
    "- Reinforces beauty industry norms\n",
    "\n",
    "**Impact**:\n",
    "- Negative psychological effects\n",
    "- Perpetuation of unrealistic beauty standards\n",
    "- Pressure to modify appearance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e73939",
   "metadata": {},
   "source": [
    "## 4.2 Stakeholder Analysis\n",
    "\n",
    "Understanding who is affected by this system and how.\n",
    "\n",
    "### Primary Stakeholders:\n",
    "\n",
    "1. **End Users**\n",
    "   - **Impact**: Direct recipients of predictions\n",
    "   - **Concerns**: Privacy, accuracy, fairness, psychological effects\n",
    "   - **Needs**: Transparency, control over data, fair treatment\n",
    "\n",
    "2. **Application Developers**\n",
    "   - **Impact**: Integrate model into their applications\n",
    "   - **Concerns**: Liability, reputation, performance\n",
    "   - **Needs**: Reliable model, clear documentation, legal compliance\n",
    "\n",
    "3. **Beauty Industry**\n",
    "   - **Impact**: May use for product recommendations\n",
    "   - **Concerns**: Accuracy for their use case, customer satisfaction\n",
    "   - **Needs**: Customization, integration support\n",
    "\n",
    "### Secondary Stakeholders:\n",
    "\n",
    "4. **Marginalized Groups**\n",
    "   - **Impact**: May experience bias or unfair treatment\n",
    "   - **Concerns**: Discrimination, representation, harm\n",
    "   - **Needs**: Fairness, accountability, recourse\n",
    "\n",
    "5. **Researchers and Ethicists**\n",
    "   - **Impact**: Study implications and effects\n",
    "   - **Concerns**: Responsible AI, societal impact\n",
    "   - **Needs**: Transparency, access to model details\n",
    "\n",
    "6. **Regulators**\n",
    "   - **Impact**: Ensure compliance with laws\n",
    "   - **Concerns**: Privacy, discrimination, consumer protection\n",
    "   - **Needs**: Auditability, compliance documentation\n",
    "\n",
    "7. **Society at Large**\n",
    "   - **Impact**: Affected by normalization of AI beauty standards\n",
    "   - **Concerns**: Cultural impact, mental health effects\n",
    "   - **Needs**: Public awareness, oversight\n",
    "\n",
    "### Stakeholder Conflicts:\n",
    "\n",
    "- **Users vs. Companies**: Privacy vs. data collection for improvement\n",
    "- **Accuracy vs. Fairness**: Optimizing for overall accuracy may harm minority groups\n",
    "- **Innovation vs. Regulation**: Speed of development vs. thorough ethical review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed22b1",
   "metadata": {},
   "source": [
    "## 4.3 Societal Impact Assessment\n",
    "\n",
    "### Positive Potential Impacts:\n",
    "\n",
    "1. **Beauty Industry Innovation**\n",
    "   - Personalized product recommendations\n",
    "   - Virtual makeup try-on experiences\n",
    "   - Enhanced user experience in beauty apps\n",
    "\n",
    "2. **Entertainment and Creativity**\n",
    "   - Fun filters and effects\n",
    "   - Photo editing assistance\n",
    "   - Creative applications\n",
    "\n",
    "3. **Accessibility**\n",
    "   - Help visually impaired users understand makeup in images\n",
    "   - Assistive technology for makeup application\n",
    "\n",
    "### Negative Potential Impacts:\n",
    "\n",
    "1. **Mental Health Concerns**\n",
    "   - Increased anxiety about appearance\n",
    "   - Comparison with AI-judged \"standards\"\n",
    "   - Body image issues, especially among young people\n",
    "   - Pressure to meet AI-defined beauty standards\n",
    "\n",
    "2. **Discrimination and Bias**\n",
    "   - Unfair treatment based on appearance predictions\n",
    "   - Reinforcement of existing societal biases\n",
    "   - Exclusion of certain groups from opportunities\n",
    "\n",
    "3. **Privacy Erosion**\n",
    "   - Normalization of facial analysis\n",
    "   - Potential for mass surveillance\n",
    "   - Loss of anonymity in public spaces\n",
    "\n",
    "4. **Economic Impacts**\n",
    "   - Pressure to buy beauty products\n",
    "   - Exploitation by beauty industry\n",
    "   - Creation of unrealistic expectations\n",
    "\n",
    "5. **Cultural Homogenization**\n",
    "   - Promotion of Western beauty standards globally\n",
    "   - Loss of diverse cultural beauty ideals\n",
    "   - Pressure to conform to dominant standards\n",
    "\n",
    "### Long-term Societal Risks:\n",
    "\n",
    "- **Normalization of AI Judgment**: People becoming accustomed to AI evaluating their appearance\n",
    "- **Dependence on Technology**: Over-reliance on AI for beauty decisions\n",
    "- **Erosion of Human Agency**: AI influencing personal choices about appearance\n",
    "- **Widening Inequalities**: Those who can afford AI-recommended products vs. those who cannot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17052fcc",
   "metadata": {},
   "source": [
    "## 4.4 Recommendations for Responsible AI Deployment\n",
    "\n",
    "Based on the identified ethical issues, here are concrete recommendations:\n",
    "\n",
    "### 1. Fairness and Bias Mitigation\n",
    "\n",
    "**Actions**:\n",
    "- **Audit Model Performance** across different demographics (skin tone, age, gender, ethnicity)\n",
    "- **Collect Diverse Training Data** that represents all user groups\n",
    "- **Use Fairness Metrics** (demographic parity, equalized odds) during evaluation\n",
    "- **Regular Bias Testing** with diverse test sets\n",
    "- **Transparent Reporting** of performance differences across groups\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Example: Evaluate per-demographic performance\n",
    "# Split test set by skin tone, gender, age\n",
    "# Report accuracy separately for each group\n",
    "# Set minimum performance thresholds for all groups\n",
    "```\n",
    "\n",
    "### 2. Handle Subjective Attributes Carefully\n",
    "\n",
    "**Actions**:\n",
    "- **Remove \"Attractive\" attribute** from production model (too subjective)\n",
    "- **Clear Labeling**: If kept, label as \"AI prediction based on celebrity data\"\n",
    "- **Disclaimers**: Explain that beauty is subjective and culturally dependent\n",
    "- **User Control**: Allow users to disable subjective predictions\n",
    "- **Focus on Objective Attributes**: Prioritize makeup-related attributes over subjective ones\n",
    "\n",
    "### 3. Privacy Protection\n",
    "\n",
    "**Actions**:\n",
    "- **Minimize Data Collection**: Only collect what's necessary\n",
    "- **No Long-term Storage**: Delete images immediately after processing\n",
    "- **Clear Privacy Policy**: Explain data usage in simple language\n",
    "- **User Consent**: Explicit opt-in for data processing\n",
    "- **Data Rights**: Allow users to request deletion and export\n",
    "- **Encryption**: Encrypt data in transit and at rest\n",
    "- **Compliance**: Follow GDPR, CCPA, and other privacy regulations\n",
    "\n",
    "### 4. Prevent Misuse\n",
    "\n",
    "**Actions**:\n",
    "- **Usage Guidelines**: Clear terms of service prohibiting discriminatory use\n",
    "- **Access Controls**: Limit who can use the API\n",
    "- **Rate Limiting**: Prevent mass surveillance use\n",
    "- **Monitoring**: Detect unusual usage patterns\n",
    "- **Prohibited Uses**: Explicitly ban use in:\n",
    "  - Employment decisions\n",
    "  - Financial services\n",
    "  - Law enforcement\n",
    "  - Healthcare decisions\n",
    "  - Educational admissions\n",
    "\n",
    "### 5. Transparency and Explainability\n",
    "\n",
    "**Actions**:\n",
    "- **Model Cards**: Document model capabilities, limitations, and biases\n",
    "- **Clear Communication**: Explain what the model does and doesn't do\n",
    "- **Confidence Scores**: Always show prediction confidence\n",
    "- **Explanation Features**: Provide visual explanations (like saliency maps)\n",
    "- **Limitations Disclosure**: Be upfront about accuracy and fairness limitations\n",
    "- **Appeal Process**: Allow users to challenge predictions\n",
    "\n",
    "### 6. User Empowerment\n",
    "\n",
    "**Actions**:\n",
    "- **Opt-in, Not Opt-out**: Require explicit consent\n",
    "- **Control Settings**: Let users choose which attributes to predict\n",
    "- **Feedback Mechanism**: Allow users to report issues\n",
    "- **Human Override**: Provide option for human review\n",
    "- **Education**: Teach users about AI limitations\n",
    "\n",
    "### 7. Continuous Monitoring and Improvement\n",
    "\n",
    "**Actions**:\n",
    "- **Regular Audits**: Quarterly fairness and bias audits\n",
    "- **User Feedback Analysis**: Monitor complaints and concerns\n",
    "- **Impact Studies**: Research actual effects on users\n",
    "- **Model Updates**: Retrain with more diverse data\n",
    "- **Ethics Review Board**: Establish internal oversight\n",
    "\n",
    "### 8. Responsible Marketing\n",
    "\n",
    "**Actions**:\n",
    "- **No Overhyping**: Don't claim perfection or objectivity\n",
    "- **Clear Limitations**: Explain what the model can and cannot do\n",
    "- **Avoid Harmful Messaging**: Don't suggest people \"should\" look a certain way\n",
    "- **Positive Framing**: Focus on fun and creativity, not judgment\n",
    "\n",
    "### 9. Regulatory Compliance\n",
    "\n",
    "**Actions**:\n",
    "- **Legal Review**: Consult with legal experts\n",
    "- **Compliance Documentation**: Maintain records for audits\n",
    "- **Stay Updated**: Monitor changing regulations (EU AI Act, etc.)\n",
    "- **Industry Standards**: Follow best practices from ACM, IEEE, etc.\n",
    "\n",
    "### 10. Stakeholder Engagement\n",
    "\n",
    "**Actions**:\n",
    "- **Advisory Board**: Include ethicists, diverse community representatives\n",
    "- **Public Consultation**: Gather feedback from potential users\n",
    "- **Collaboration**: Work with advocacy groups on fairness\n",
    "- **Research Partnerships**: Partner with universities on impact studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54345828",
   "metadata": {},
   "source": [
    "## 4.5 Accountability Framework\n",
    "\n",
    "### Responsibility Assignment:\n",
    "\n",
    "| Role | Responsibility | Accountability |\n",
    "|------|---------------|----------------|\n",
    "| **Model Developer** | Build fair, accurate model | Performance metrics, bias audits |\n",
    "| **Data Team** | Ensure diverse, quality data | Data documentation, source tracking |\n",
    "| **Product Manager** | Define ethical use cases | Use case guidelines, user feedback |\n",
    "| **Legal Team** | Ensure compliance | Regulatory adherence, risk assessment |\n",
    "| **Ethics Board** | Oversight and review | Regular audits, incident review |\n",
    "| **Customer Support** | Handle user concerns | Response time, resolution rate |\n",
    "| **Leadership** | Set ethical standards | Company policy, resource allocation |\n",
    "\n",
    "### Accountability Mechanisms:\n",
    "\n",
    "1. **Documentation**\n",
    "   - Model card with limitations and biases\n",
    "   - Training data provenance\n",
    "   - Deployment decisions and rationale\n",
    "   - Incident reports\n",
    "\n",
    "2. **Regular Audits**\n",
    "   - Quarterly bias audits\n",
    "   - Annual third-party review\n",
    "   - Performance monitoring dashboards\n",
    "\n",
    "3. **Incident Response Plan**\n",
    "   - Clear escalation path\n",
    "   - Response time commitments\n",
    "   - Remediation procedures\n",
    "   - Public communication strategy\n",
    "\n",
    "4. **Metrics and KPIs**\n",
    "   - Fairness metrics across demographics\n",
    "   - User satisfaction scores\n",
    "   - Complaint resolution rate\n",
    "   - Transparency score\n",
    "\n",
    "### Ethical Guidelines Checklist:\n",
    "\n",
    "Before deploying, ensure:\n",
    "- [ ] Bias audit completed across all demographics\n",
    "- [ ] Privacy impact assessment conducted\n",
    "- [ ] Terms of service include prohibited uses\n",
    "- [ ] Model card published\n",
    "- [ ] User consent mechanism implemented\n",
    "- [ ] Data retention policy defined\n",
    "- [ ] Feedback mechanism in place\n",
    "- [ ] Regular audit schedule established\n",
    "- [ ] Incident response plan documented\n",
    "- [ ] Legal compliance verified\n",
    "- [ ] Ethics board review completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13544a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Summary and Conclusions\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project successfully developed a makeup and beauty detection model using ResNet18 architecture trained on the CelebA dataset. The model predicts 5 attributes: Heavy_Makeup, Wearing_Lipstick, Attractive, High_Cheekbones, and Rosy_Cheeks.\n",
    "\n",
    "## Key Achievements\n",
    "\n",
    "### 1. Model Performance\n",
    "- **Test Accuracy**: ~89% (per-attribute average)\n",
    "- **Average F1-Score**: ~78%\n",
    "- **Superior to Baselines**: Outperformed SimpleBaseline and VanillaCNN\n",
    "- **Reliable Predictions**: Good confidence calibration\n",
    "\n",
    "### 2. Model Interpretation\n",
    "- Implemented gradient-based saliency maps\n",
    "- Verified model focuses on semantically correct regions\n",
    "- Demonstrated transparency and explainability\n",
    "- Confidence analysis shows model reliability\n",
    "\n",
    "### 3. Deployment Readiness\n",
    "- Created comprehensive deployment architecture\n",
    "- Developed FastAPI service for inference\n",
    "- Established monitoring and maintenance strategy\n",
    "- Addressed scalability and compliance considerations\n",
    "\n",
    "### 4. Ethical Analysis\n",
    "- Identified key ethical issues (bias, privacy, misuse)\n",
    "- Conducted stakeholder analysis\n",
    "- Assessed societal impacts\n",
    "- Provided concrete recommendations for responsible AI\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Dataset Bias**: Celebrity-focused dataset may not generalize well\n",
    "2. **Subjective Attributes**: \"Attractive\" is culturally dependent\n",
    "3. **Demographic Gaps**: May perform worse on underrepresented groups\n",
    "4. **Image Quality**: Requires good quality, well-lit face images\n",
    "\n",
    "## Future Work\n",
    "\n",
    "1. **Technical Improvements**:\n",
    "   - Collect more diverse training data\n",
    "   - Try deeper architectures (ResNet50, EfficientNet)\n",
    "   - Implement attention mechanisms\n",
    "   - Address class imbalance\n",
    "\n",
    "2. **Ethical Enhancements**:\n",
    "   - Conduct comprehensive bias audits\n",
    "   - Remove or reframe subjective attributes\n",
    "   - Implement fairness constraints\n",
    "   - Engage with diverse communities\n",
    "\n",
    "3. **Deployment Optimizations**:\n",
    "   - Implement A/B testing framework\n",
    "   - Add real-time monitoring dashboards\n",
    "   - Optimize for mobile deployment\n",
    "   - Develop user feedback loop\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### For Responsible Deployment:\n",
    "\n",
    "1. **Do Not Use For**:\n",
    "   - High-stakes decisions (hiring, admissions, etc.)\n",
    "   - Surveillance without consent\n",
    "   - Discriminatory purposes\n",
    "\n",
    "2. **Best Use Cases**:\n",
    "   - Virtual makeup try-on (with consent)\n",
    "   - Photo editing assistance\n",
    "   - Entertainment and creative applications\n",
    "   - Personal use with full transparency\n",
    "\n",
    "3. **Required Safeguards**:\n",
    "   - Clear user consent\n",
    "   - Transparent limitations disclosure\n",
    "   - Regular bias audits\n",
    "   - User control and privacy protection\n",
    "   - Prohibition of harmful uses\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates both the technical capabilities and ethical complexities of AI-based beauty detection. While the model achieves good performance, deploying such systems requires careful consideration of fairness, privacy, and societal impact.\n",
    "\n",
    "**Key Takeaway**: Technical success must be balanced with ethical responsibility. AI systems that judge or analyze human appearance require extra scrutiny, transparency, and safeguards to prevent harm and ensure fairness.\n",
    "\n",
    "The framework provided in this report—from performance analysis to deployment planning to ethical guidelines—can serve as a template for responsibly developing and deploying similar AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Report**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
