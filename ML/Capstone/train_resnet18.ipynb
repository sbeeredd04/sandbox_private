{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba916634",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4493054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759b05a",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Image size: 224x224\n",
      "  Batch size: 512\n",
      "  Epochs: 20\n",
      "  Learning rate: 0.001\n",
      "  Attributes: ['Heavy_Makeup', 'Wearing_Lipstick', 'Attractive', 'High_Cheekbones', 'Rosy_Cheeks']\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "image_size = 224\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "data_dir = './data'\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Selected attributes\n",
    "selected_attributes = ['Heavy_Makeup', 'Wearing_Lipstick', 'Attractive', 'High_Cheekbones', 'Rosy_Cheeks']\n",
    "num_attributes = len(selected_attributes)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image size: {image_size}x{image_size}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Attributes: {selected_attributes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fb7e1",
   "metadata": {},
   "source": [
    "## 3. Data Transforms and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54542d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data transforms defined\n"
     ]
    }
   ],
   "source": [
    "# Data transforms with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Validation transform (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "print(\"✓ Data transforms defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6e915d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CelebA dataset...\n",
      "✓ Training samples: 162770\n",
      "✓ Validation samples: 19867\n",
      "\n",
      "Attribute indices: [18, 36, 2, 19, 29]\n"
     ]
    }
   ],
   "source": [
    "# Load CelebA dataset\n",
    "print(\"Loading CelebA dataset...\")\n",
    "\n",
    "try:\n",
    "    train_dataset = datasets.CelebA(\n",
    "        root=data_dir,\n",
    "        split='train',\n",
    "        transform=train_transform,\n",
    "        download=False,\n",
    "        target_type='attr'\n",
    "    )\n",
    "    \n",
    "    val_dataset = datasets.CelebA(\n",
    "        root=data_dir,\n",
    "        split='valid',\n",
    "        transform=val_transform,\n",
    "        download=False,\n",
    "        target_type='attr'\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "    print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please ensure CelebA dataset is in the data directory\")\n",
    "    raise\n",
    "\n",
    "# Get attribute names and indices\n",
    "attribute_names = [name for name in train_dataset.attr_names if name.strip()]\n",
    "attribute_indices = [attribute_names.index(attr) for attr in selected_attributes]\n",
    "print(f\"\\nAttribute indices: {attribute_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40208615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset filtering applied\n"
     ]
    }
   ],
   "source": [
    "# Replace the AttributeFilterDataset cell with this:\n",
    "from dataset import AttributeFilterDataset\n",
    "\n",
    "# Wrap datasets\n",
    "train_dataset = AttributeFilterDataset(train_dataset, attribute_indices)\n",
    "val_dataset = AttributeFilterDataset(val_dataset, attribute_indices)\n",
    "\n",
    "print(\"✓ Dataset filtering applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48ce2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoaders created\n",
      "  Training batches: 318\n",
      "  Validation batches: 39\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True  # Keeps workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"✓ DataLoaders created\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0435e54",
   "metadata": {},
   "source": [
    "## 4. Define ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed24b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BasicBlock defined\n"
     ]
    }
   ],
   "source": [
    "# Basic block for ResNet18\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic residual block with skip connection\"\"\"\n",
    "    \n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"✓ BasicBlock defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d44ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ResNet18MultiLabel defined\n"
     ]
    }
   ],
   "source": [
    "# ResNet18 for multi-label classification\n",
    "class ResNet18MultiLabel(nn.Module):\n",
    "    \"\"\"ResNet18 architecture for multi-label classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ResNet18MultiLabel, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # ResNet layers (2, 2, 2, 2 blocks)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"✓ ResNet18MultiLabel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ec0c5",
   "metadata": {},
   "source": [
    "## 5. Create Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48f1bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model created with 11,179,077 parameters\n",
      "✓ Loss: BCEWithLogitsLoss\n",
      "✓ Optimizer: Adam (lr=0.001, weight_decay=0.0001)\n",
      "✓ Scheduler: ReduceLROnPlateau\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = ResNet18MultiLabel(num_classes=num_attributes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "print(f\"✓ Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"✓ Loss: BCEWithLogitsLoss\")\n",
    "print(f\"✓ Optimizer: Adam (lr={learning_rate}, weight_decay={weight_decay})\")\n",
    "print(f\"✓ Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644a2a4",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16afe935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ train_epoch function defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        predictions = torch.sigmoid(output) > 0.5\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_targets.append(target.cpu())\n",
    "\n",
    "        # Progress\n",
    "        print(f'\\rEpoch {epoch}: [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}', end='', flush=True)\n",
    "\n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    all_predictions = torch.cat(all_predictions).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    epoch_acc = (all_predictions == all_targets).mean()\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"✓ train_epoch function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e57f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ validate_epoch function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.sigmoid(output) > 0.5\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_targets.append(target.cpu())\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    all_predictions = torch.cat(all_predictions).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    val_acc = (all_predictions == all_targets).mean()\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "print(\"✓ validate_epoch function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daffad3",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "================================================================================\n",
      "Epoch 1: [54/318] Loss: 0.5066"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'\\n{\"-\"*80}')\n",
    "    print(f'Epoch {epoch}/{num_epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "    print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "    print(f'  LR: {optimizer.param_groups[0][\"lr \"]:.6f}')\n",
    "    print(f'{\"-\"*80}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_celeba_resnet18.pth')\n",
    "        print(f'  ✓ New best model saved with Val Acc: {best_val_acc:.4f}')\n",
    "        \n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
